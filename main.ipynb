{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28f347d-1233-4bf2-8dbf-520bf48d87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip\n",
    "# !ls -lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3651dd61-fcbc-4925-87d3-abb9caea41ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vladimiragishev/mambaforge/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ebabae-37a2-40ef-9cc2-b2761d407cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available. Training on GPU ...')\n",
    "    device = torch.device(\"cuda\") \n",
    "# elif torch.has_mps:\n",
    "#     print('Apple ARM is available. Training on ARM')\n",
    "#     device = torch.device(\"mps\") \n",
    "else: \n",
    "    print('CUDA is not available. Training on CPU ...')\n",
    "    device = torch.device(\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8911d53d-74c9-451a-9d1d-ca85df6b0ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "with open('embeddings/glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        count +=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "115f863a-e88e-4f40-8e41-687a3d0d7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preparation():\n",
    "    \n",
    "    \"\"\"\n",
    "    Preparation dataset of news\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, target: pd.Series, mode: str, bert=False):\n",
    "        \n",
    "        pd.options.mode.chained_assignment = None\n",
    "        data = data.reset_index(drop=True)\n",
    "        target = target.reset_index(drop=True)\n",
    "        \n",
    "        data['target'] = target \n",
    "        self.max_length = 256\n",
    "        self.data = data\n",
    "        self.mode = mode\n",
    "        self.bert = bert\n",
    "        self.target = target\n",
    "        self.len_ = len(data)\n",
    "        \n",
    "        DATA_MODES = ['train', 'test']\n",
    "        if self.mode not in DATA_MODES:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
    "            raise NameError\n",
    "\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        if not(self.bert):\n",
    "            self.glove_model = {}\n",
    "            with open('embeddings/glove.6B.100d.txt','r') as f:\n",
    "                for line in f:\n",
    "                    split_line = line.split()\n",
    "                    word = split_line[0]\n",
    "                    embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "                    self.glove_model[word] = embedding\n",
    "                \n",
    "    def filter_(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        \n",
    "        \"\"\"\n",
    "        Main data preparation, create from str dataframe numpy ndarray\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        data = data.to_numpy()\n",
    "        print('Replacing data with embeddings...')\n",
    "        \n",
    "        if self.bert:\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "            data_null = np.zeros((data.shape[0], self.max_length))\n",
    "            data_attention = np.zeros((data.shape[0], self.max_length))\n",
    "        else:\n",
    "            data_null = np.zeros((data.shape[0], self.max_length, 100))\n",
    "        \n",
    "        for row_index in trange(data.shape[0]):\n",
    "            \n",
    "            news_text = data[row_index][1] + data[row_index][2]\n",
    "            news_text = self.str_transforms_(news_text)\n",
    "            news_text = news_text[:self.max_length]\n",
    "            \n",
    "            if self.bert:\n",
    "                news_text = ' '.join(news_text)\n",
    "                bert_input = tokenizer(news_text, padding='max_length', max_length = self.max_length, \n",
    "                                truncation=True, return_tensors=\"pt\")\n",
    "                \n",
    "                data_null[row_index] = bert_input['input_ids']\n",
    "                data_attention[row_index] = bert_input['attention_mask']\n",
    "                    \n",
    "            else:\n",
    "                if len(news_text) < self.max_length:\n",
    "                    to_add = self.max_length - len(news_text)\n",
    "                    padding = ['@None'] * to_add\n",
    "                    news_text = news_text + padding\n",
    "                \n",
    "                for word_index in range(len(news_text)):\n",
    "                    try: word_embedding = self.glove_model[news_text[word_index]]\n",
    "                    except: word_embedding = np.zeros(100)\n",
    "                    data_null[row_index][word_index] = word_embedding\n",
    "                    \n",
    "        if not(self.bert):\n",
    "            data_null = np.swapaxes(data_null, 1, 2)\n",
    "            return data_null, 0\n",
    "        \n",
    "        return data_null, data_attention\n",
    "        \n",
    "    def str_transforms_(self, news_text: str) -> list:\n",
    "        \n",
    "        \"\"\"\n",
    "        Cleaning str from .,!, stopwords and lemmatize it\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        news_text = news_text.lower()\n",
    "        list_of_char = ['.', ',', '!', '?', '&']\n",
    "        pattern = '[' + ''.join(list_of_char) + ']'\n",
    "        news_text = re.sub(pattern, '', news_text)\n",
    "        news_text = ' '.join([i for i in news_text.split( ) if i not in self.stop_words])\n",
    "        word_list = nltk.word_tokenize(news_text)\n",
    "        news_text = ' '.join([self.lemmatizer.lemmatize(news_text) for w in word_list])\n",
    "        news_text = news_text.split()\n",
    "        \n",
    "        return news_text\n",
    "        \n",
    "    def create_vocab_(self, data: np.ndarray):\n",
    "        \n",
    "        \"\"\"\n",
    "        Create vocab of ints for embedding in bert\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        vocab = {}\n",
    "        all_text_str = ''\n",
    "        for row_index in range(data.shape[0]):\n",
    "                news_text = data[row_index][1] + data[row_index][2]\n",
    "                news_text = self.str_transforms_(news_text)\n",
    "                news_text = ' '.join(news_text)\n",
    "                all_text_str = all_text_str + ' ' + news_text\n",
    "        \n",
    "        index = 0\n",
    "        for element in list(set(all_text_str.split())):\n",
    "            vocab[element] = index\n",
    "            index += 1\n",
    "        vocab['@None'] = index\n",
    "        \n",
    "        return vocab\n",
    "        \n",
    "    def data_augmentation_(self, data):\n",
    "        \n",
    "        \"\"\"\n",
    "        Make data augmentation with synonyms replacement and translation to German and back\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        labels = self.target \n",
    "        count_to_add = labels.count() - labels.sum()\n",
    "        \n",
    "        data_to_augmentate = data.iloc[labels[labels == 1].index]\n",
    "        \n",
    "        try: data_to_augmentate = pd.concat([data_to_augmentate] * (count_to_add // labels.sum()), ignore_index=True)\n",
    "        except: return data, data['target']\n",
    "    \n",
    "        aug = naw.SynonymAug(aug_src='wordnet')\n",
    "        random_indexes = data_to_augmentate.sample(frac = 0.8).index\n",
    "        \n",
    "        print('Augmentate data with synonyms replace...')\n",
    "        for index in tqdm(random_indexes):\n",
    "            data_to_augmentate.iloc[index]['title']   = aug.augment(str(data_to_augmentate.iloc[random_indexes]['title']))\n",
    "            data_to_augmentate.iloc[index]['summary'] = aug.augment(str(data_to_augmentate.iloc[random_indexes]['summary']))\n",
    "        \n",
    "        random_indexes = data_to_augmentate.sample(frac = 0.4).index\n",
    "        trasns_ger = GoogleTranslator(source = 'auto', target = 'de')\n",
    "        trasns_en  = GoogleTranslator(source = 'auto', target = 'en')\n",
    "        \n",
    "        print('Augmentate data with translation to deutch and back...')\n",
    "        for index in tqdm(random_indexes):\n",
    "            data_to_augmentate.iloc[index]['title']   = trasns_ger.translate(str(data_to_augmentate.iloc[random_indexes]['title']))\n",
    "            data_to_augmentate.iloc[index]['summary'] = trasns_ger.translate(str(data_to_augmentate.iloc[random_indexes]['summary']))\n",
    "            \n",
    "            data_to_augmentate.iloc[index]['title']   = trasns_en.translate(str(data_to_augmentate.iloc[random_indexes]['title']))\n",
    "            data_to_augmentate.iloc[index]['summary'] = trasns_en.translate(str(data_to_augmentate.iloc[random_indexes]['summary']))\n",
    "            \n",
    "        data = pd.concat([data, data_to_augmentate])\n",
    "        print(\"One augmentated sentence:\", data_to_augmentate.tail(1)['summary'])\n",
    "        \n",
    "        return data, data['target']\n",
    "        \n",
    "    def transform(self, save=False, save_name = \"prepeared_data\"):\n",
    " \n",
    "        \"\"\"\n",
    "        Main transform data function\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        if self.mode == 'test':\n",
    "            y = self.target\n",
    "            x, attention = self.filter_(self.data)\n",
    "        \n",
    "        else:\n",
    "            x, y = self.data_augmentation_(self.data)\n",
    "            x, attention = self.filter_(x)\n",
    "            \n",
    "        if save:\n",
    "            with open('data/' + save_name + '.npy', 'wb') as f:\n",
    "                np.save(f, x)\n",
    "                np.save(f, y)  \n",
    "                np.save(f, attention) \n",
    "            \n",
    "        return x, attention, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff45e8e-62cd-4f06-99be-0d5e89cd72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Preparation dataset of news\n",
    "    \"\"\"\n",
    "    def __init__(self, data, attention, target, mode):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = attention\n",
    "        self.data = data\n",
    "        self.target = target.to_numpy()\n",
    "        self.len_ = len(data)\n",
    "        self.mode = mode\n",
    "\n",
    "        DATA_MODES = ['train', 'val', 'test']\n",
    "        if self.mode not in DATA_MODES:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
    "            raise NameError\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.len_\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        \n",
    "        x = self.data\n",
    "        y = self.target\n",
    "        \n",
    "        try:\n",
    "            attention = self.attention[index]\n",
    "        except:\n",
    "            attention = 0\n",
    "            \n",
    "        if self.mode == \"test\":\n",
    "            return x[index], attention\n",
    "        else:\n",
    "            return x[index], attention, y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e5177b1-7b17-45f4-9eaa-074310a5fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/data_augmentated_train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"data/data_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "6fc54704-fa64-4072-8687-0de55f022b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('target', axis=1)\n",
    "y_train = train_df['target']\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(test_df.drop('target', axis=1), test_df['target'], \n",
    "                                                          test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "53257956-f9de-4f34-ae8f-0fb4e9e647d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing data with embeddings...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006933927536010742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5206,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2099674fd6654d52b79d29838737f9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentate data with synonyms replace...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007058143615722656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 343,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1966ea287e4f258d6352893921b0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vladimiragishev/mambaforge/lib/python3.9/site-packages/pandas/core/series.py:1056: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cacher_needs_updating = self._check_is_chained_assignment_possible()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentate data with translation to deutch and back...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0062007904052734375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 172,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf9f74cbd894b70b0341a4b5884d9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One augmentated sentence: 428    After nearly two years of unrest, the company ...\n",
      "Name: summary, dtype: object\n",
      "Replacing data with embeddings...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019082069396972656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 871,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406b4cbf66774d7fa5c97eb5dd3f6f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/871 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing data with embeddings...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006403207778930664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 442,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3a49b163a84a8a9238ed36c665282f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, _,  y_train = Preparation(data=X_train, target=y_train, mode='train').transform()\n",
    "X_val, _,  y_val     = Preparation(data=X_val, target=y_val, mode='train').transform()\n",
    "X_test, _,  y_test   = Preparation(data=X_test, target=y_test, mode='test').transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "e6c8de8f-b3b3-4fe8-a915-aee9609a6ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5206, 100, 256), (871, 100, 256), (442, 100, 256))"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b9622b-0d58-4c10-ae5d-fdb30421a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    processed_data = 0\n",
    "    \n",
    "    for inputs, attention, labels in train_loader:\n",
    "        if device.type == 'mps':\n",
    "            inputs = inputs.to(dtype=torch.float32).to(device)\n",
    "            labels = labels.to(dtype=torch.float32).to(device)\n",
    "        else:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.double()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if type(attention) == int:\n",
    "            outputs = model(inputs)\n",
    "        else:\n",
    "            outputs = model(inputs.long(), mask=attention.to(device))\n",
    "            \n",
    "        labels = labels.unsqueeze(1).to(torch.float64)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        processed_data += inputs.size(0)\n",
    "        print(preds)\n",
    "        break\n",
    "    train_loss = running_loss / processed_data\n",
    "    train_f1 = f1_score(labels.cpu().data, preds.cpu())\n",
    "    return train_loss, train_f1\n",
    "\n",
    "def eval_epoch(model, val_loader, creterion, optimizer):\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    processed_data = 0\n",
    "    \n",
    "    for inputs, attention, labels in val_loader:\n",
    "        if device.type == 'mps':\n",
    "            inputs = inputs.to(dtype=torch.float32).to(device)\n",
    "            labels = labels.to(dtype=torch.float32).to(device)\n",
    "        else:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.double()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            labels = labels.unsqueeze(1).to(torch.float64)\n",
    "            \n",
    "            if type(attention) == int:\n",
    "                outputs = model(inputs)\n",
    "            else:\n",
    "                outputs = model(inputs.long(), mask=attention.to(device))\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        processed_data += inputs.size(0)\n",
    "              \n",
    "    val_loss = running_loss / processed_data\n",
    "    val_f1 = f1_score(labels.cpu().data, preds.cpu())\n",
    "    return val_loss, val_f1\n",
    "\n",
    "def train(model, train_dataset, val_dataset, num_epochs, lr, batch_size):\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    logs = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_f1 {t_f1:0.4f} val_f1 {v_f1:0.4f}\"\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    with tqdm(desc='epoch', total=num_epochs) as pbar_outer:\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            train_loss, train_f1 = fit_epoch(model, train_loader, criterion, optimizer)\n",
    "            val_loss, val_f1 = fit_epoch(model, val_loader, criterion, optimizer)\n",
    "            \n",
    "            logs.append((train_loss, train_f1, val_loss, val_f1))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_f1=train_f1, v_f1=val_f1))\n",
    "            \n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03634505-1150-4838-a206-8681f6c386c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv1d(100, 200, 3, stride=1)\n",
    "        self.conv_2 = nn.Conv1d(200, 400, 3, stride=1)\n",
    "        self.conv_3 = nn.Conv1d(400, 800, 3, stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(6400, 3200)\n",
    "        self.fc2 = nn.Linear(3200, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.max_pool = nn.AvgPool1d(3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.max_pool(self.relu(self.conv_1(x)))\n",
    "        x = self.max_pool(self.relu(self.conv_2(x)))\n",
    "        x = self.max_pool(self.relu(self.conv_3(x))).flatten(start_dim=1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigm(self.fc4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "69c5f75c-3abd-4d98-b51d-95322c4fe606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Convolutional(\n",
       "  (conv_1): Conv1d(100, 200, kernel_size=(3,), stride=(1,))\n",
       "  (conv_2): Conv1d(200, 400, kernel_size=(3,), stride=(1,))\n",
       "  (conv_3): Conv1d(400, 800, kernel_size=(3,), stride=(1,))\n",
       "  (fc1): Linear(in_features=6400, out_features=3200, bias=True)\n",
       "  (fc2): Linear(in_features=3200, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.01)\n",
       "  (sigm): Sigmoid()\n",
       "  (max_pool): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       ")"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn = Convolutional()\n",
    "model_cnn = model_cnn.to(device)\n",
    "\n",
    "model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "dbe9bd41-c187-43bd-925b-caa937e2a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = NewsDataset(X_val, 0, y_val, 'val')\n",
    "train_dataset= NewsDataset(X_train, 0, y_train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "94a438f9-33eb-4ffa-a714-fb7f64fda59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016815185546875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "epoch",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25a5411e7cf4116aa4927187554e6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 0.6962     val_loss 0.6931 train_f1 0.0000 val_f1 0.0000\n",
      "\n",
      "Epoch 002 train_loss: 0.6931     val_loss 0.6931 train_f1 0.0000 val_f1 0.0000\n",
      "\n",
      "Epoch 003 train_loss: 0.6931     val_loss 0.6931 train_f1 0.0000 val_f1 0.0000\n",
      "\n",
      "Epoch 004 train_loss: 0.6931     val_loss 0.6931 train_f1 0.0000 val_f1 0.0000\n",
      "\n",
      "Epoch 005 train_loss: 0.6931     val_loss 0.6931 train_f1 0.0000 val_f1 0.0000\n"
     ]
    }
   ],
   "source": [
    "history_cnn = train(model_cnn, train_dataset, val_dataset, 5, 0.0001, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "5d9bca77-b0c1-4788-b412-13c43c3f094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentRNN,self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(256, 100, 20)\n",
    "        self.linear = nn.Linear(10000, 1)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x, hidden = self.lstm(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = self.sigm(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "5b23500a-fd2e-4b89-a2e7-917f49216a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (lstm): LSTM(256, 100, num_layers=20)\n",
       "  (linear): Linear(in_features=10000, out_features=1, bias=True)\n",
       "  (sigm): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm = SentimentRNN()\n",
    "model_lstm = model_lstm.to(device)\n",
    "\n",
    "model_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "91017181-45fa-4bd8-be4b-0d1b4f3c442e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008275985717773438,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "epoch",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75612ac5c0f43a9a17a96c6e374d65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 0.7007     val_loss 0.6933 train_f1 0.0000 val_f1 0.0000\n",
      "\n",
      "Epoch 002 train_loss: 0.6934     val_loss 0.6923 train_f1 0.0000 val_f1 0.0000\n",
      "\n",
      "Epoch 003 train_loss: 0.6933     val_loss 0.6931 train_f1 0.0000 val_f1 0.0000\n",
      "\n",
      "Epoch 004 train_loss: 0.6938     val_loss 0.6928 train_f1 0.0000 val_f1 0.0000\n",
      "\n",
      "Epoch 005 train_loss: 0.6933     val_loss 0.6922 train_f1 0.0000 val_f1 0.0000\n"
     ]
    }
   ],
   "source": [
    "history_lstm = train(model_lstm, train_dataset, val_dataset, 5, 0.0001, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14de1f91-dc04-4436-842a-90de0eec501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bert = train_df.drop('target', axis=1)\n",
    "y_train_bert = train_df['target']\n",
    "\n",
    "X_val_bert, X_test_bert, y_val_bert, y_test_bert = train_test_split(test_df.drop('target', axis=1), test_df['target'], \n",
    "                                                                    test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3ea4ef-64ad-465d-b361-101a4f689c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing data with embeddings...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016390323638916016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5206,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bee0a9e20440878a253933359299c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentate data with synonyms replace...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007148027420043945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 343,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8349d9d0bb1e427399245309c96d149d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentate data with translation to deutch and back...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00652313232421875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 20,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 172,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2304f5b605041d9bc3328e27d415583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pd/dc92r6ps4_q25drxrf_7btlm0000gn/T/ipykernel_97639/934676787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m X_train_bert, X_train_attention_bert, y_train_bert = Preparation(data=X_train_bert, target=y_train_bert, \n\u001b[1;32m      2\u001b[0m                                                     mode='train', bert=True).transform()\n\u001b[0;32m----> 3\u001b[0;31m X_val_bert, X_val_attention_bert, y_val_bert = Preparation(data=X_val_bert, target=y_val_bert, \n\u001b[0m\u001b[1;32m      4\u001b[0m                                                     mode='train', bert=True).transform()\n\u001b[1;32m      5\u001b[0m X_test_bert, X_test_attention_bert, y_test_bert = Preparation(data=X_test_bert, target=y_test_bert, \n",
      "\u001b[0;32m/var/folders/pd/dc92r6ps4_q25drxrf_7btlm0000gn/T/ipykernel_97639/196731232.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, save, save_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_augmentation_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/pd/dc92r6ps4_q25drxrf_7btlm0000gn/T/ipykernel_97639/196731232.py\u001b[0m in \u001b[0;36mdata_augmentation_\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mdata_to_augmentate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtrasns_ger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_augmentate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mdata_to_augmentate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrasns_ger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_augmentate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mdata_to_augmentate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtrasns_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_augmentate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/deep_translator/google.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             response = requests.get(\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             )\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \"\"\"\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_chunk_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_bert, X_train_attention_bert, y_train_bert = Preparation(data=X_train_bert, target=y_train_bert, \n",
    "                                                    mode='train', bert=True).transform()\n",
    "X_val_bert, X_val_attention_bert, y_val_bert = Preparation(data=X_val_bert, target=y_val_bert, \n",
    "                                                    mode='train', bert=True).transform()\n",
    "X_test_bert, X_test_attention_bert, y_test_bert = Preparation(data=X_test_bert, target=y_test_bert, \n",
    "                                                    mode='test', bert=True).transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef4603-88ff-4ffe-a2ae-cc04983c9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bert.shape, X_test_attention_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb9a8f-b15f-415f-96aa-33c1ef60b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.relu = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n",
    "    \n",
    "model_bert = BertClassifier()\n",
    "\n",
    "model_bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155746db-ac1a-4dbd-a6fc-1653f47ce0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_bert = NewsDataset(X_train_bert, X_train_attention_bert, y_train_bert, 'train')\n",
    "val_dataset_bert = NewsDataset(X_val_bert, X_val_attention_bert, y_val_bert, 'val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c7d14-ac1f-4cbb-bf64-27ca46a22355",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_bert = train(model_bert, train_dataset_bert, val_dataset_bert, 5, 0.0001, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced93654-ec1d-48c4-87f1-44a05bc808d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
